# DSPy API Environment Variables

# Port configuration - separate from main API server
PORT=5005

# HuggingFace API configuration
# Replace with your own API key for production use
HUGGINGFACE_API_KEY=your_huggingface_api_key_here
HUGGINGFACE_MODEL=microsoft/Phi-4-mini-reasoning

# Student model configuration
STUDENT_MODEL=google/flan-t5-small

# Training data path
DSPY_TRAINING_DATA=data/processed/dspy_training_data/qa_dataset.jsonl

# Model save path
DSPY_MODEL_PATH=models/dspy

# LM Studio API endpoints
LM_STUDIO_API_URL=http://localhost:1234/v1
LM_STUDIO_EMBEDDING_MODEL=text-embedding-nomic-embed-text-v1.5@q8_0
LM_STUDIO_CHAT_MODEL=your_chat_model_name_here
LM_STUDIO_COMPLETION_MODEL=your_completion_model_name_here

# Disable Langfuse integration (fixes "NoneType has no len()" errors)
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
LANGFUSE_HOST=https://localhost:1234

# DSPy Configuration
# Replace with your actual API keys and configuration

# LLM Provider Keys (add your provider of choice)
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
HUGGINGFACE_API_KEY=your_huggingface_api_key_here
COHERE_API_KEY=your_cohere_api_key_here

# LM Studio Configuration (for local models)
LM_STUDIO_URL=http://localhost:1234/v1
LM_STUDIO_MODEL=your_local_model_here

# MLflow Tracking
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=bible_qa_optimization

# DSPy Optimization Parameters
DSPY_NUM_THREADS=4
DSPY_CACHE_DIR=./local_cache/dspy
DSPY_VERBOSE=True
DSPY_TIMEOUT=120

# Model Configuration
DSPY_LM=claude-3-sonnet
DSPY_RM=your_retrieval_model_here
DSPY_EMBEDDING_MODEL=text-embedding-3-large
DSPY_T5_MODEL_PATH=./models/dspy/bible_qa_t5

# Training Configuration
TRAIN_BATCH_SIZE=16
EVAL_BATCH_SIZE=8
LEARNING_RATE=5e-5
NUM_EPOCHS=3
MAX_LENGTH=512
WARMUP_STEPS=500
GRADIENT_ACCUMULATION_STEPS=2

# Validation Dataset
VALIDATION_SPLIT=0.1
RANDOM_SEED=42 